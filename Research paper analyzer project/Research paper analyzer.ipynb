{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-14T09:33:53.920002Z",
     "iopub.status.busy": "2025-09-14T09:33:53.919810Z",
     "iopub.status.idle": "2025-09-14T09:33:54.194195Z",
     "shell.execute_reply": "2025-09-14T09:33:54.193414Z",
     "shell.execute_reply.started": "2025-09-14T09:33:53.919985Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/epilepsy/Epileptic_Seizure_Detection_in_EEG_Signals_Using_Machine_Learning_and_Deep_Learning_Techniques.pdf\n",
      "/kaggle/input/research/s00259-020-05108-y.pdf\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:35:29.179279Z",
     "iopub.status.busy": "2025-09-14T09:35:29.178624Z",
     "iopub.status.idle": "2025-09-14T09:35:29.590825Z",
     "shell.execute_reply": "2025-09-14T09:35:29.590018Z",
     "shell.execute_reply.started": "2025-09-14T09:35:29.179257Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9143b6a371d847dcb86b5bf1c6dc203a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:35:45.271999Z",
     "iopub.status.busy": "2025-09-14T09:35:45.271426Z",
     "iopub.status.idle": "2025-09-14T09:35:50.921881Z",
     "shell.execute_reply": "2025-09-14T09:35:50.921165Z",
     "shell.execute_reply.started": "2025-09-14T09:35:45.271972Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n",
      "Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:35:55.851199Z",
     "iopub.status.busy": "2025-09-14T09:35:55.850395Z",
     "iopub.status.idle": "2025-09-14T09:37:46.874341Z",
     "shell.execute_reply": "2025-09-14T09:37:46.873685Z",
     "shell.execute_reply.started": "2025-09-14T09:35:55.851166Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.66)\n",
      "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.7.0)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Collecting langchain-core\n",
      "  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.14.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain-community) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m868.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: requests, python-dotenv, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, httpx-sse, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pydantic-settings, nvidia-cusolver-cu12, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.66\n",
      "    Uninstalling langchain-core-0.3.66:\n",
      "      Successfully uninstalled langchain-core-0.3.66\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.8\n",
      "    Uninstalling langchain-text-splitters-0.3.8:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.8\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.26\n",
      "    Uninstalling langchain-0.3.26:\n",
      "      Successfully uninstalled langchain-0.3.26\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.1 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed httpx-sse-0.4.1 langchain-0.3.27 langchain-community-0.3.29 langchain-core-0.3.76 langchain-text-splitters-0.3.11 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydantic-settings-2.10.1 python-dotenv-1.1.1 requests-2.32.5\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-core pypdf sentence-transformers transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:53.396745Z",
     "iopub.status.busy": "2025-09-14T09:37:53.396457Z",
     "iopub.status.idle": "2025-09-14T09:37:56.397445Z",
     "shell.execute_reply": "2025-09-14T09:37:56.396746Z",
     "shell.execute_reply.started": "2025-09-14T09:37:53.396722Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:37:56.399310Z",
     "iopub.status.busy": "2025-09-14T09:37:56.398673Z",
     "iopub.status.idle": "2025-09-14T09:37:59.668695Z",
     "shell.execute_reply": "2025-09-14T09:37:59.667954Z",
     "shell.execute_reply.started": "2025-09-14T09:37:56.399273Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
      "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate sentencepiece PyPDF2 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:38:33.408199Z",
     "iopub.status.busy": "2025-09-14T09:38:33.407090Z",
     "iopub.status.idle": "2025-09-14T09:38:33.412349Z",
     "shell.execute_reply": "2025-09-14T09:38:33.411535Z",
     "shell.execute_reply.started": "2025-09-14T09:38:33.408171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import requests\n",
    "import io\n",
    "import PyPDF2\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:38:35.227858Z",
     "iopub.status.busy": "2025-09-14T09:38:35.227076Z",
     "iopub.status.idle": "2025-09-14T09:43:12.840182Z",
     "shell.execute_reply": "2025-09-14T09:43:12.839546Z",
     "shell.execute_reply.started": "2025-09-14T09:38:35.227825Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e866a28967640c09c02c42b02aa1cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/181k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a41b2709074f809e3b33bec53f85f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9f40c85a29449c9467af472d1668ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e357e8348613482b8af3a06ba3e3729b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/622 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2626920b68424e56ba741d2ce2a27844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/29.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a985fa69ec453d99392c0fb332fcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb94b3526bd47968fd4ee2632f6d38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058a8a2a0da640bf9c54a5a53ac8e25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434d297eb2384d878382796f2981a197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e3bbc83d384f07acf643db2f1c8c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9a5e1908c140dead29fe971ee2bede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236d720f285d4bd39a1cedc11436b724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74cd1630fec4e208873e76a8582a50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:44:59.705869Z",
     "iopub.status.busy": "2025-09-14T09:44:59.705532Z",
     "iopub.status.idle": "2025-09-14T09:44:59.710783Z",
     "shell.execute_reply": "2025-09-14T09:44:59.709984Z",
     "shell.execute_reply.started": "2025-09-14T09:44:59.705847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=300):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:45:04.178690Z",
     "iopub.status.busy": "2025-09-14T09:45:04.178133Z",
     "iopub.status.idle": "2025-09-14T09:45:12.143457Z",
     "shell.execute_reply": "2025-09-14T09:45:12.142480Z",
     "shell.execute_reply.started": "2025-09-14T09:45:04.178665Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fitz\n",
      "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl.metadata (816 bytes)\n",
      "Collecting configobj (from fitz)\n",
      "  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting configparser (from fitz)\n",
      "  Downloading configparser-7.2.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: httplib2 in /usr/local/lib/python3.11/dist-packages (from fitz) (0.22.0)\n",
      "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (from fitz) (5.3.2)\n",
      "Collecting nipype (from fitz)\n",
      "  Downloading nipype-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fitz) (1.26.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from fitz) (2.2.3)\n",
      "Collecting pyxnat (from fitz)\n",
      "  Downloading pyxnat-1.6.3-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from fitz) (1.15.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2->fitz) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel->fitz) (6.5.2)\n",
      "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel->fitz) (25.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel->fitz) (4.14.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->fitz) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->fitz) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->fitz) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->fitz) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->fitz) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->fitz) (2.4.1)\n",
      "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (8.2.1)\n",
      "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.5)\n",
      "Collecting prov>=1.5.2 (from nipype->fitz)\n",
      "  Downloading prov-2.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (2.9.0.post0)\n",
      "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
      "  Downloading rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: simplejson>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.20.1)\n",
      "Collecting traits>=6.2 (from nipype->fitz)\n",
      "  Downloading traits-7.0.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (3.18.0)\n",
      "Collecting acres (from nipype->fitz)\n",
      "  Downloading acres-0.5.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting etelemetry>=0.3.1 (from nipype->fitz)\n",
      "  Downloading etelemetry-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting looseversion!=1.2 (from nipype->fitz)\n",
      "  Downloading looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: puremagic in /usr/local/lib/python3.11/dist-packages (from nipype->fitz) (1.29)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->fitz) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->fitz) (2025.2)\n",
      "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.11/dist-packages (from pyxnat->fitz) (5.4.0)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from pyxnat->fitz) (2.32.5)\n",
      "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.11/dist-packages (from pyxnat->fitz) (1.0.1)\n",
      "Collecting ci-info>=0.2 (from etelemetry>=0.3.1->nipype->fitz)\n",
      "  Downloading ci_info-0.3.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.2->nipype->fitz) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->pyxnat->fitz) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->pyxnat->fitz) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->pyxnat->fitz) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->pyxnat->fitz) (2025.6.15)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->fitz) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->fitz) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->fitz) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->fitz) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->fitz) (2024.2.0)\n",
      "Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
      "Downloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n",
      "Downloading configparser-7.2.0-py3-none-any.whl (17 kB)\n",
      "Downloading nipype-1.10.0-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyxnat-1.6.3-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.4/95.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
      "Downloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading prov-2.1.1-py3-none-any.whl (425 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.9/425.9 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rdflib-7.1.4-py3-none-any.whl (565 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.1/565.1 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading traits-7.0.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading acres-0.5.0-py3-none-any.whl (12 kB)\n",
      "Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
      "Installing collected packages: looseversion, traits, rdflib, configparser, configobj, ci-info, acres, pyxnat, prov, etelemetry, nipype, fitz\n",
      "Successfully installed acres-0.5.0 ci-info-0.3.0 configobj-5.0.9 configparser-7.2.0 etelemetry-0.3.1 fitz-0.0.1.dev2 looseversion-1.3.0 nipype-1.10.0 prov-2.1.1 pyxnat-1.6.3 rdflib-7.1.4 traits-7.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T00:50:40.686580Z",
     "iopub.status.busy": "2025-09-14T00:50:40.686326Z",
     "iopub.status.idle": "2025-09-14T00:50:44.730502Z",
     "shell.execute_reply": "2025-09-14T00:50:44.729643Z",
     "shell.execute_reply.started": "2025-09-14T00:50:40.686562Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:45:17.648302Z",
     "iopub.status.busy": "2025-09-14T09:45:17.647349Z",
     "iopub.status.idle": "2025-09-14T09:45:17.654976Z",
     "shell.execute_reply": "2025-09-14T09:45:17.654261Z",
     "shell.execute_reply.started": "2025-09-14T09:45:17.648264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_abstract(text):\n",
    "    \n",
    "    markers = [\n",
    "        (\"Abstract\", \"Introduction\"),\n",
    "        (\"ABSTRACT\", \"1. Introduction\"),\n",
    "        (\"Summary\", \"Keywords\"),\n",
    "        (\"\\nAbstract\\n\", \"\\n1 \"),\n",
    "        (\"abstract\\n\", \"\\n1 \"),\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    for start_marker, end_marker in markers:\n",
    "        start_marker_lower = start_marker.lower()\n",
    "        end_marker_lower = end_marker.lower()\n",
    "        \n",
    "        if start_marker_lower in text_lower and end_marker_lower in text_lower:\n",
    "            start_idx = text_lower.index(start_marker_lower) + len(start_marker)\n",
    "            end_idx = text_lower.index(end_marker_lower, start_idx)\n",
    "            return text[start_idx:end_idx].strip()\n",
    "    \n",
    "    return text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:45:20.369735Z",
     "iopub.status.busy": "2025-09-14T09:45:20.369016Z",
     "iopub.status.idle": "2025-09-14T09:45:20.375593Z",
     "shell.execute_reply": "2025-09-14T09:45:20.374784Z",
     "shell.execute_reply.started": "2025-09-14T09:45:20.369709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_topics(text):\n",
    "    abstract = extract_abstract(text)[:1500]  \n",
    "    \n",
    "    prompt = f\"\"\"<s>[INST] Extract the 5 most specific research topics from this abstract.\n",
    "    - Return ONLY the topics as bullet points (- topic)\n",
    "    - Each topic should be 2-5 words maximum\n",
    "    - Exclude generic terms like 'analysis' or 'study'\n",
    "    - Do NOT include any examples or explanations\n",
    "    \n",
    "    ABSTRACT: {abstract} [/INST]\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.3,  \n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "   \n",
    "    topics = []\n",
    "    for line in response.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line.startswith('-'):\n",
    "            topic = line[1:].strip()\n",
    "            if 2 <= len(topic.split()) <= 5:  \n",
    "                topics.append(topic)\n",
    "    \n",
    "    return topics[:5]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:45:23.534773Z",
     "iopub.status.busy": "2025-09-14T09:45:23.534011Z",
     "iopub.status.idle": "2025-09-14T09:45:37.348572Z",
     "shell.execute_reply": "2025-09-14T09:45:37.347726Z",
     "shell.execute_reply.started": "2025-09-14T09:45:23.534746Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 300 chars of extracted text: ORIGINAL ARTICLE\n",
      "A deep learning framework for18F-FDG PET imaging diagnosis\n",
      "in pediatric patients with temporal lobe epilepsy\n",
      "Qinming Zhang1,2&Yi Liao1,3&Xiawan Wang1,3&Teng Zhang1,3&Jianhua Feng4&Jianing Deng2&Kexin Shi1,3&\n",
      "Lin Chen1&Liu Feng1,3&Mindi Ma1,3&Le Xue1,3&Haifeng Hou1&Xiaofeng Dou1&Cong ...\n",
      "\n",
      "Extracted Topics:\n",
      "1. Symmetricity-Driven Deep Learning Framework\n",
      "2. Pediatric Patients with Epipsey\n",
      "3. Tempora Loble Epipsy (Pediatric)\n",
      "4. Symmetry Features (366)\n",
      "5. Pair-of-Cube Based\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "# 5. التنفيذ\n",
    "pdf_path = \"/kaggle/input/research/s00259-020-05108-y.pdf\"\n",
    "paper_text = extract_text_from_pdf(pdf_path)\n",
    "print(\"First 300 chars of extracted text:\", paper_text[:300], \"...\")  # للتحقق\n",
    "\n",
    "topics = extract_topics(paper_text)\n",
    "print(\"\\nExtracted Topics:\")\n",
    "for i, topic in enumerate(topics, 1):\n",
    "    print(f\"{i}. {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:50:18.212081Z",
     "iopub.status.busy": "2025-09-14T09:50:18.211762Z",
     "iopub.status.idle": "2025-09-14T09:50:18.219544Z",
     "shell.execute_reply": "2025-09-14T09:50:18.218830Z",
     "shell.execute_reply.started": "2025-09-14T09:50:18.212061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "#1) search arXiv \n",
    "def search_arxiv_by_topic(topics, max_results=3):\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    ns = {\"atom\": \"http://www.w3.org/2005/Atom\"}\n",
    "    all_results = {}\n",
    "\n",
    "    for topic in topics:\n",
    "        params = {\n",
    "            \"search_query\": topic,\n",
    "            \"start\": 0,\n",
    "            \"max_results\": max_results\n",
    "        }\n",
    "        r = requests.get(base_url, params=params, timeout=15)\n",
    "        if r.status_code != 200:\n",
    "            all_results[topic] = [{\"title\": \"Error fetching results\", \"link\": \"\", \"abstract\": \"\"}]\n",
    "            continue\n",
    "\n",
    "        root = ET.fromstring(r.content)\n",
    "        results = []\n",
    "        for entry in root.findall(\"atom:entry\", ns):\n",
    "            title_el = entry.find(\"atom:title\", ns)\n",
    "            id_el    = entry.find(\"atom:id\", ns)\n",
    "            summary_el= entry.find(\"atom:summary\", ns)\n",
    "\n",
    "            title = title_el.text.strip() if title_el is not None and title_el.text else \"No title\"\n",
    "            abs_txt = summary_el.text.strip() if summary_el is not None and summary_el.text else \"\"\n",
    "            raw_id = id_el.text.strip() if id_el is not None and id_el.text else \"\"\n",
    "            # make pdf link from abs link\n",
    "            if raw_id:\n",
    "                pdf_link = raw_id.replace(\"/abs/\", \"/pdf/\")\n",
    "            else:\n",
    "                pdf_link = \"\"\n",
    "\n",
    "            results.append({\"title\": title, \"link\": pdf_link, \"abstract\": abs_txt})\n",
    "\n",
    "        all_results[topic] = results if results else [{\"title\": \"No results found\", \"link\": \"\", \"abstract\": \"\"}]\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:50:21.174699Z",
     "iopub.status.busy": "2025-09-14T09:50:21.173976Z",
     "iopub.status.idle": "2025-09-14T09:50:21.180166Z",
     "shell.execute_reply": "2025-09-14T09:50:21.179622Z",
     "shell.execute_reply.started": "2025-09-14T09:50:21.174674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def summarize_abstract_with_mistral_or_fallback(text, max_len=400):\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return \" No abstract text available.\"\n",
    "\n",
    "    prompt = f\"Read the following abstract and provide a concise summary (not more than 3 bullet points). Avoid copying sentences directly:\\n\\n{text}\"\n",
    "\n",
    "    # if generate_text (Mistral) exists in globals, use it\n",
    "    if \"generate_text\" in globals():\n",
    "        try:\n",
    "            # generate_text returns a list of strings per your earlier code\n",
    "            out = generate_text(prompt, max_length=max_len)\n",
    "            if isinstance(out, list) and out:\n",
    "                return out[0].strip()\n",
    "            elif isinstance(out, str):\n",
    "                return out.strip()\n",
    "        except Exception as e:\n",
    "            # fall through to HF fallback\n",
    "            fallback_error = f\"(Mistral generate failed: {e})\\n\"\n",
    "\n",
    "    # Fallback: use HuggingFace summarizer (light handling)\n",
    "    try:\n",
    "        from transformers import pipeline\n",
    "        summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "        # keep within model input limits — chunk if needed\n",
    "        chunk = text[:3000]\n",
    "        s = summarizer(chunk, max_length=80, min_length=50, do_sample=False)\n",
    "        return s[0][\"summary_text\"].strip()\n",
    "    except Exception as e:\n",
    "        return f\" Summarization failed: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:50:24.830052Z",
     "iopub.status.busy": "2025-09-14T09:50:24.829450Z",
     "iopub.status.idle": "2025-09-14T09:50:24.834483Z",
     "shell.execute_reply": "2025-09-14T09:50:24.833850Z",
     "shell.execute_reply.started": "2025-09-14T09:50:24.830029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  3) Main flow\n",
    "def run_search_and_summarize(topics, max_results_per_topic=3):\n",
    "    papers_by_topic = search_arxiv_by_topic(topics, max_results=max_results_per_topic)\n",
    "    final = []  # list of dicts {topic, title, link, abstract, summary}\n",
    "\n",
    "    for topic, papers in papers_by_topic.items():\n",
    "        for paper in papers:\n",
    "            title = paper.get(\"title\",\"\")\n",
    "            link  = paper.get(\"link\",\"\")\n",
    "            abstract = paper.get(\"abstract\",\"\")\n",
    "            summary = summarize_abstract_with_mistral_or_fallback(abstract)\n",
    "            final.append({\n",
    "                \"topic\": topic,\n",
    "                \"title\": title,\n",
    "                \"link\": link,\n",
    "                \"abstract\": abstract,\n",
    "                \"summary\": summary\n",
    "            })\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:51:06.856125Z",
     "iopub.status.busy": "2025-09-14T09:51:06.855401Z",
     "iopub.status.idle": "2025-09-14T09:53:17.969217Z",
     "shell.execute_reply": "2025-09-14T09:53:17.968332Z",
     "shell.execute_reply.started": "2025-09-14T09:51:06.856101Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Topic: Symmetricity-Driven Deep Learning Framework\n",
      "🔗 Title: Moving Deep Learning into Web Browser: How Far Can We Go?\n",
      "🔗 PDF link: http://arxiv.org/pdf/1901.09388v2\n",
      "📝 Abstract (first 300 chars): Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers. However, little is known on what and how well we can do with these frameworks for deep learning in browsers. To bridge the knowledge gap, in this paper, w ...\n",
      "✅ Summary:\n",
      " Read the following abstract and provide a concise summary (not more than 3 bullet points). Avoid copying sentences directly:\n",
      "\n",
      "Recently, several JavaScript-based deep learning frameworks have emerged,\n",
      "making it possible to perform deep learning tasks directly in browsers.\n",
      "However, little is known on what and how well we can do with these frameworks\n",
      "for deep learning in browsers. To bridge the knowledge gap, in this paper, we\n",
      "conduct the first empirical study of deep learning in browsers. We survey 7\n",
      "most popular JavaScript-based deep learning frameworks, investigating to what\n",
      "extent deep learning tasks have been supported in browsers so far. Then we\n",
      "measure the performance of different frameworks when running different deep\n",
      "learning tasks. Finally, we dig out the performance gap between deep learning\n",
      "in browsers and on native platforms by comparing the performance of\n",
      "TensorFlow.js and TensorFlow in Python. Our findings could help application\n",
      "developers, deep-learning framework vendors and browser vendors to improve the\n",
      "efficiency of deep learning in browsers. (Source: arXiv.org)\n",
      "\n",
      "- The study explores the capabilities and performance of JavaScript-based deep learning frameworks in browsers.\n",
      "- It surveys 7 popular frameworks and measures their performance on various deep learning tasks.\n",
      "- The study also compares the performance of TensorFlow.js (JavaScript) and TensorFlow (Python) to highlight the gap between browser-based and native deep learning.\n",
      "============================================================\n",
      "\n",
      "📌 Topic: Symmetricity-Driven Deep Learning Framework\n",
      "🔗 Title: A Unified Framework of Deep Neural Networks by Capsules\n",
      "🔗 PDF link: http://arxiv.org/pdf/1805.03551v2\n",
      "📝 Abstract (first 300 chars): With the growth of deep learning, how to describe deep neural networks unifiedly is becoming an important issue. We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphs ...\n",
      "✅ Summary:\n",
      " Read the following abstract and provide a concise summary (not more than 3 bullet points). Avoid copying sentences directly:\n",
      "\n",
      "With the growth of deep learning, how to describe deep neural networks\n",
      "unifiedly is becoming an important issue. We first formalize neural networks\n",
      "mathematically with their directed graph representations, and prove a\n",
      "generation theorem about the induced networks of connected directed acyclic\n",
      "graphs. Then, we set up a unified framework for deep learning with capsule\n",
      "networks. This capsule framework could simplify the description of existing\n",
      "deep neural networks, and provide a theoretical basis of graphic designing and\n",
      "programming techniques for deep learning models, thus would be of great\n",
      "significance to the advancement of deep learning. We also provide a\n",
      "demonstration of our framework with a simple example.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "- The authors address the need for a unified description of deep neural networks (DNNs) as they become more complex.\n",
      "- They propose a mathematical formalization using directed graph representations and prove a generation theorem for connected directed acyclic graphs.\n",
      "- The authors introduce a unified framework for deep learning based on capsule networks, which simplifies the description of existing DNNs and provides a theoretical basis for designing and programming deep learning models.\n",
      "============================================================\n",
      "\n",
      "📌 Topic: Pediatric Patients with Epipsey\n",
      "🔗 Title: Artificial Intelligence for Pediatric Ophthalmology\n",
      "🔗 PDF link: http://arxiv.org/pdf/1904.08796v1\n",
      "📝 Abstract (first 300 chars): PURPOSE OF REVIEW: Despite the impressive results of recent artificial intelligence (AI) applications to general ophthalmology, comparatively less progress has been made toward solving problems in pediatric ophthalmology using similar techniques. This article discusses the unique needs of pediatric  ...\n",
      "✅ Summary:\n",
      " Read the following abstract and provide a concise summary (not more than 3 bullet points). Avoid copying sentences directly:\n",
      "\n",
      "PURPOSE OF REVIEW: Despite the impressive results of recent artificial\n",
      "intelligence (AI) applications to general ophthalmology, comparatively less\n",
      "progress has been made toward solving problems in pediatric ophthalmology using\n",
      "similar techniques. This article discusses the unique needs of pediatric\n",
      "ophthalmology patients and how AI techniques can address these challenges,\n",
      "surveys recent applications of AI to pediatric ophthalmology, and discusses\n",
      "future directions in the field.\n",
      "  RECENT FINDINGS: The most significant advances involve the automated\n",
      "detection of retinopathy of prematurity (ROP), yielding results that rival\n",
      "experts. Machine learning (ML) has also been successfully applied to the\n",
      "classification of pediatric cataracts, prediction of post-operative\n",
      "complications following cataract surgery, detection of strabismus and\n",
      "refractive error, prediction of future high myopia, and diagnosis of reading\n",
      "disability via eye tracking. In addition, ML techniques have been used for the\n",
      "study of visual development, vessel segmentation in pediatric fundus images,\n",
      "and ophthalmic image synthesis.\n",
      "  SUMMARY: AI applications could significantly benefit clinical care for\n",
      "pediatric ophthalmology patients by optimizing disease detection and grading,\n",
      "broadening access to care, furthering scientific discovery, and improving\n",
      "clinical efficiency. These methods need to match or surpass physician\n",
      "performance in clinical trials before deployment with patients. Due to\n",
      "widespread use of closed-access data sets and software implementations, it is\n",
      "difficult to directly compare the performance of these approaches, and\n",
      "reproducibility is poor. Open-access data sets and software implementations\n",
      "could alleviate these issues, and encourage further AI applications to\n",
      "pediatric ophthalmology.\n",
      "  KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence,\n",
      "deep learning, retinopathy of prematurity, strabismus, cataract, myopia,\n",
      "refractive error, reading disability, visual development, ophthalmic image\n",
      "synthesis, vessel segmentation\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "- **AI in Pediatric Ophthalmology:** AI, particularly machine learning (ML), has shown promising results in various pediatric ophthalmology applications, such as detecting retinopathy of prematurity (ROP), classifying cataracts, predicting post-operative complications, and diagnosing reading disabilities.\n",
      "- **Benefits of AI:** AI applications could significantly improve clinical care for pediatric ophthalmology patients by enhancing disease detection and grading, increasing access to care, aiding scientific discovery, and boosting clinical efficiency.\n",
      "- **Challenges and Future Directions:** Before widespread use, AI methods must match or surpass physician performance in clinical trials. To facilitate comparison and reproducibility, open-access data sets and software implementations are needed to encourage further AI applications in pediatric ophthalmology.\n",
      "============================================================\n",
      "\n",
      "📌 Topic: Pediatric Patients with Epipsey\n",
      "🔗 Title: Deep Learning Prediction of Severe Health Risks for Pediatric COVID-19\n",
      "  Patients with a Large Feature Set in 2021 BARDA Data Challenge\n",
      "🔗 PDF link: http://arxiv.org/pdf/2206.01696v2\n",
      "📝 Abstract (first 300 chars): Most children infected with COVID-19 have no or mild symptoms and can recover automatically by themselves, but some pediatric COVID-19 patients need to be hospitalized or even to receive intensive medical care (e.g., invasive mechanical ventilation or cardiovascular support) to recover from the illn ...\n",
      "✅ Summary:\n",
      " Read the following abstract and provide a concise summary (not more than 3 bullet points). Avoid copying sentences directly:\n",
      "\n",
      "Most children infected with COVID-19 have no or mild symptoms and can recover\n",
      "automatically by themselves, but some pediatric COVID-19 patients need to be\n",
      "hospitalized or even to receive intensive medical care (e.g., invasive\n",
      "mechanical ventilation or cardiovascular support) to recover from the\n",
      "illnesses. Therefore, it is critical to predict the severe health risk that\n",
      "COVID-19 infection poses to children to provide precise and timely medical care\n",
      "for vulnerable pediatric COVID-19 patients. However, predicting the severe\n",
      "health risk for COVID-19 patients including children remains a significant\n",
      "challenge because many underlying medical factors affecting the risk are still\n",
      "largely unknown. In this work, instead of searching for a small number of most\n",
      "useful features to make prediction, we design a novel large-scale bag-of-words\n",
      "like method to represent various medical conditions and measurements of\n",
      "COVID-19 patients. After some simple feature filtering based on logistical\n",
      "regression, the large set of features is used with a deep learning method to\n",
      "predict both the hospitalization risk for COVID-19 infected children and the\n",
      "severe complication risk for the hospitalized pediatric COVID-19 patients. The\n",
      "method was trained and tested the datasets of the Biomedical Advanced Research\n",
      "and Development Authority (BARDA) Pediatric COVID-19 Data Challenge held from\n",
      "Sept. 15 to Dec. 17, 2021. The results show that the approach can rather\n",
      "accurately predict the risk of hospitalization and severe complication for\n",
      "pediatric COVID-19 patients and deep learning is more accurate than other\n",
      "machine learning methods. The method can be used to help medical professionals\n",
      "to provide precise and timely medical care for vulnerable pediatric COVID-19\n",
      "patients.\n",
      "\n",
      "- The study aims to predict the risk of severe COVID-19 in children to enable timely medical intervention.\n",
      "- A novel bag-of-words method is used to represent various medical conditions and measurements of COVID-19 patients.\n",
      "- The deep learning model outperformed other machine learning methods in predicting hospitalization and severe complication risks for pediatric COVID-19 patients.\n",
      "============================================================\n",
      "\n",
      "📌 Topic: Tempora Loble Epipsy (Pediatric)\n",
      "🔗 Title: YourBench: Easy Custom Evaluation Sets for Everyone\n",
      "🔗 PDF link: http://arxiv.org/pdf/2504.01833v1\n",
      "📝 Abstract (first 300 chars): Evaluating large language models (LLMs) effectively remains a critical bottleneck, as traditional static benchmarks suffer from saturation and contamination, while human evaluations are costly and slow. This hinders timely or domain-specific assessment, crucial for real-world applications. We introd ...\n",
      "✅ Summary:\n",
      " Read the following abstract and provide a concise summary (not more than 3 bullet points). Avoid copying sentences directly:\n",
      "\n",
      "Evaluating large language models (LLMs) effectively remains a critical\n",
      "bottleneck, as traditional static benchmarks suffer from saturation and\n",
      "contamination, while human evaluations are costly and slow. This hinders timely\n",
      "or domain-specific assessment, crucial for real-world applications. We\n",
      "introduce YourBench, a novel, open-source framework that addresses these\n",
      "limitations by enabling dynamic, automated generation of reliable, up-to-date,\n",
      "and domain-tailored benchmarks cheaply and without manual annotation, directly\n",
      "from user-provided documents. We demonstrate its efficacy by replicating 7\n",
      "diverse MMLU subsets using minimal source text, achieving this for under 15 USD\n",
      "in total inference costs while perfectly preserving the relative model\n",
      "performance rankings (Spearman Rho = 1) observed on the original benchmark. To\n",
      "ensure that YourBench generates data grounded in provided input instead of\n",
      "relying on posterior parametric knowledge in models, we also introduce\n",
      "Tempora-0325, a novel dataset of over 7K diverse documents, published\n",
      "exclusively after March 2025. Our comprehensive analysis spans 26 SoTA models\n",
      "from 7 major families across varying scales (3-671B parameters) to validate the\n",
      "quality of generated evaluations through rigorous algorithmic checks (e.g.,\n",
      "citation grounding) and human assessments. We release the YourBench library,\n",
      "the Tempora-0325 dataset, 150k+ question answer pairs based on Tempora and all\n",
      "evaluation and inference traces to facilitate reproducible research and empower\n",
      "the community to generate bespoke benchmarks on demand, fostering more relevant\n",
      "and trustworthy LLM evaluation.\n",
      "============================================================\n",
      "\n",
      "📌 Topic: Tempora Loble Epipsy (Pediatric)\n",
      "🔗 Title: PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark\n",
      "🔗 PDF link: http://arxiv.org/pdf/2508.16439v3\n",
      "📝 Abstract (first 300 chars): Large language models (LLMs) and vision-augmented LLMs (VLMs) have significantly advanced medical informatics, diagnostics, and decision support. However, these models exhibit systematic biases, particularly age bias, compromising their reliability and equity. This is evident in their poorer perform ...\n",
      "✅ Summary:\n",
      " Read the following abstract and provide a concise summary (not more than 3 bullet points). Avoid copying sentences directly:\n",
      "\n",
      "Large language models (LLMs) and vision-augmented LLMs (VLMs) have\n",
      "significantly advanced medical informatics, diagnostics, and decision support.\n",
      "However, these models exhibit systematic biases, particularly age bias,\n",
      "compromising their reliability and equity. This is evident in their poorer\n",
      "performance on pediatric-focused text and visual question-answering tasks. This\n",
      "bias reflects a broader imbalance in medical research, where pediatric studies\n",
      "receive less funding and representation despite the significant disease burden\n",
      "in children. To address these issues, a new comprehensive multi-modal pediatric\n",
      "question-answering benchmark, PediatricsMQA, has been introduced. It consists\n",
      "of 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric\n",
      "topics across seven developmental stages (prenatal to adolescent) and 2,067\n",
      "vision-based MCQs using 634 pediatric images from 67 imaging modalities and 256\n",
      "anatomical regions. The dataset was developed using a hybrid manual-automatic\n",
      "pipeline, incorporating peer-reviewed pediatric literature, validated question\n",
      "banks, existing benchmarks, and existing QA resources. Evaluating\n",
      "state-of-the-art open models, we find dramatic performance drops in younger\n",
      "cohorts, highlighting the need for age-aware methods to ensure equitable AI\n",
      "support in pediatric care. The PediatricsMQA benchmark is publicly available to\n",
      "encourage further research and development in this critical area.\n",
      "\n",
      "- **Bias in LLMs and VLMs**: These models show age bias, performing poorly on pediatric tasks due to underrepresentation in medical research.\n",
      "- **New benchmark dataset**: PediatricsMQA, a comprehensive multi-modal pediatric question-answering benchmark, has been introduced to address this issue. It consists of 3,417 text-based MCQs and 2,067 vision-based MCQs covering various pediatric topics and developmental stages.\n",
      "- **Performance gap and need for age-aware methods**: Evaluations reveal significant performance drops in younger cohorts, emphasizing the necessity for age-aware methods to ensure equitable AI support in pediatric care.\n",
      "============================================================\n",
      "\n",
      "📌 Topic: Symmetry Features (366)\n",
      "🔗 Title: Lorentz covariance of the canonical perfect lens\n",
      "🔗 PDF link: http://arxiv.org/pdf/physics/0701256v1\n",
      "📝 Abstract (first 300 chars): The canonical perfect lens--comprising three slabs, each made of a linear, homogeneous, bianisotropic material with orthorhombic symmetry--is Lorentz covariant. ...\n",
      "✅ Summary:\n",
      " Read the following abstract and provide a concise summary (not more than 3 bullet points). Avoid copying sentences directly:\n",
      "\n",
      "The canonical perfect lens--comprising three slabs, each made of a linear,\n",
      "homogeneous, bianisotropic material with orthorhombic symmetry--is Lorentz\n",
      "covariant. This lens can focus electromagnetic waves to a spot size smaller than\n",
      "the wavelength, with a resolution limited only by diffraction. The lens is\n",
      "achieved by solving the Maxwell's equations in the frequency domain, using\n",
      "the constitutive relations for bianisotropic materials. The solution reveals\n",
      "that the lens can focus waves with arbitrary polarization, and the focal spot\n",
      "size is independent of the frequency. The lens also exhibits a unique property\n",
      "of being achromatic, meaning it can focus waves of different frequencies to\n",
      "the same spot.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "- The \"canonical perfect lens\" is a lens design using three slabs of bianisotropic material with orthorhombic symmetry, which is Lorentz covariant.\n",
      "- This lens can focus electromagnetic waves to a spot size smaller than the wavelength, limited only by diffraction, and can handle waves of arbitrary polarization.\n",
      "- The lens is achromatic, meaning it can focus waves of different frequencies to the same spot, making it unique in its properties.\n",
      "============================================================\n",
      "\n",
      "📌 Topic: Symmetry Features (366)\n",
      "🔗 Title: Supersymmetric U(1)B x U(1)L model with leptophilic and leptophobic cold\n",
      "  dark matters\n",
      "🔗 PDF link: http://arxiv.org/pdf/1012.4679v2\n",
      "📝 Abstract (first 300 chars): We consider a supersymmetric model with extra $U(1)_B \\times U(1)_L$ gauge symmetry that are broken spontaneously. Salient features of this model are that there are three different types of cold dark matter (CDM) candidates, and neutral scalar sector has a rich structure. Light CDM with $\\sigma_{\\rm ...\n",
      "✅ Summary:\n",
      " Read the following abstract and provide a concise summary (not more than 3 bullet points). Avoid copying sentences directly:\n",
      "\n",
      "We consider a supersymmetric model with extra $U(1)_B \\times U(1)_L$ gauge\n",
      "symmetry that are broken spontaneously. Salient features of this model are that\n",
      "there are three different types of cold dark matter (CDM) candidates, and\n",
      "neutral scalar sector has a rich structure. Light CDM with $\\sigma_{\\rm SI}\n",
      "\\sim 10^{-3\\pm 1}$ pb can be easily accommodated by leptophobic dark matter\n",
      "($\\chi_B$) with correct relic density, if the $U(1)_B$ gauge boson mass is\n",
      "around $2 m_{\\chi_B}$. Also the PAMELA and Fermi/LAT data can be fit by\n",
      "leptophilic CDM with mass $\\sim 1$ TeV. There could be interesting signatures\n",
      "of new fermions and new gauge bosons at the LHC. The model also predicts\n",
      "significant indirect detection signals in the form of $\\gamma$-rays from the\n",
      "Galactic Center and dwarf spheroidal galaxies.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "- The model introduces three types of cold dark matter (CDM) candidates and a complex neutral scalar sector.\n",
      "- It proposes leptophobic dark matter ($\\chi_B$) with a scattering cross-section of $\\sim 10^{-3\\pm 1}$ pb, which can fit the correct relic density when the $U(1)_B$ gauge boson mass is around $2 m_{\\chi_B}$.\n",
      "- The model also suggests leptophilic CDM with a mass of $\\sim 1$ TeV to explain PAMELA and Fermi/LAT data, and predicts potential signatures of new fermions and gauge bosons at the LHC, as well as significant indirect detection signals in the form of $\\gamma$-rays from the Galactic Center and dwarf spheroidal galaxies.\n",
      "============================================================\n",
      "\n",
      "📌 Topic: Pair-of-Cube Based\n",
      "🔗 Title: On special partial types and weak canonical bases in simple theories\n",
      "🔗 PDF link: http://arxiv.org/pdf/1311.3058v1\n",
      "📝 Abstract (first 300 chars): We define a notion of a weak canonical base for a partial type. This notion is weaker than the usual canonical base for an amalgamation base. We prove that certain family of partial types have a weak canonical base. This family clearly contains the class of amalgamation bases. ...\n",
      "✅ Summary:\n",
      " Read the following abstract and provide a concise summary (not more than 3 bullet points). Avoid copying sentences directly:\n",
      "\n",
      "We define a notion of a weak canonical base for a partial type. This notion\n",
      "is weaker than the usual canonical base for an amalgamation base. We prove that\n",
      "certain family of partial types have a weak canonical base. This family clearly\n",
      "contains the class of amalgamation bases. We also prove that the weak canonical\n",
      "base is unique up to isomorphism.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "- The authors introduce a new concept called a \"weak canonical base\" for partial types, which is less restrictive than the traditional \"canonical base\" for amalgamation bases.\n",
      "- They demonstrate that a specific family of partial types, which includes amalgamation bases, possesses this weak canonical base.\n",
      "- The weak canonical base is shown to be unique, up to isomorphism.\n",
      "============================================================\n",
      "\n",
      "📌 Topic: Pair-of-Cube Based\n",
      "🔗 Title: On Randomization-based and Regression-based Inferences for 2^K Factorial\n",
      "  Designs\n",
      "🔗 PDF link: http://arxiv.org/pdf/1602.03972v1\n",
      "📝 Abstract (first 300 chars): We extend the randomization-based causal inference framework in Dasgupta et al. (2015) for general 2^K factorial designs, and demonstrate the equivalence between regression-based and randomization-based inferences. Consequently, we justify the use of regression-based methods in 2^K factorial designs ...\n",
      "✅ Summary:\n",
      " Read the following abstract and provide a concise summary (not more than 3 bullet points). Avoid copying sentences directly:\n",
      "\n",
      "We extend the randomization-based causal inference framework in Dasgupta et\n",
      "al. (2015) for general 2^K factorial designs, and demonstrate the equivalence\n",
      "between regression-based and randomization-based inferences. Consequently, we\n",
      "justify the use of regression-based methods in 2^K factorial designs from a\n",
      "finite-population perspective. We also propose a novel approach to estimate\n",
      "the variance of the treatment effects, which is crucial for constructing\n",
      "confidence intervals and performing hypothesis tests.\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "- The authors expand the causal inference framework from Dasgupta et al. (2015) to accommodate 2^K factorial designs.\n",
      "- They prove that regression-based and randomization-based inferences are equivalent in these designs, validating the use of regression-based methods.\n",
      "- The authors introduce a new method to estimate the variance of treatment effects, enabling the construction of confidence intervals and hypothesis testing.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 4) Run \n",
    "topics = globals().get(\"topics\", [\"deep learning\", \"epilepsy\"])  # if you already have `topics` from Mistral it will be used\n",
    "results = run_search_and_summarize(topics, max_results_per_topic=2)\n",
    "\n",
    "# ----- 5) Print nicely -----\n",
    "for r in results:\n",
    "    print(\"\\n📌 Topic:\", r[\"topic\"])\n",
    "    print(\"🔗 Title:\", r[\"title\"])\n",
    "    print(\"🔗 PDF link:\", r[\"link\"])\n",
    "    print(\"📝 Abstract (first 300 chars):\", (r[\"abstract\"] or \"\")[:300].replace(\"\\n\", \" \"), \"...\")\n",
    "    print(\"✅ Summary:\\n\", r[\"summary\"])\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:53:54.230403Z",
     "iopub.status.busy": "2025-09-14T09:53:54.230093Z",
     "iopub.status.idle": "2025-09-14T09:53:54.234487Z",
     "shell.execute_reply": "2025-09-14T09:53:54.233686Z",
     "shell.execute_reply.started": "2025-09-14T09:53:54.230381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NGROK_TOKEN = \"\"\n",
    "API_KEY = \"Test11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:53:56.782115Z",
     "iopub.status.busy": "2025-09-14T09:53:56.781804Z",
     "iopub.status.idle": "2025-09-14T09:54:00.347342Z",
     "shell.execute_reply": "2025-09-14T09:54:00.346576Z",
     "shell.execute_reply.started": "2025-09-14T09:53:56.782094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
      "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: pyngrok\n",
      "Successfully installed pyngrok-7.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:54:04.607002Z",
     "iopub.status.busy": "2025-09-14T09:54:04.606191Z",
     "iopub.status.idle": "2025-09-14T09:54:05.661280Z",
     "shell.execute_reply": "2025-09-14T09:54:05.660513Z",
     "shell.execute_reply.started": "2025-09-14T09:54:04.606973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request, HTTPException\n",
    "import uvicorn, threading, time, socket\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def gen(req: Request):\n",
    "    if req.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n",
    "    data = await req.json()\n",
    "    return {\n",
    "        \"response\": generate_text(\n",
    "            data.get(\"prompt\", \"\"),\n",
    "            data.get(\"max_length\", 300)\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T09:54:08.981449Z",
     "iopub.status.busy": "2025-09-14T09:54:08.981171Z",
     "iopub.status.idle": "2025-09-14T09:54:11.549305Z",
     "shell.execute_reply": "2025-09-14T09:54:11.548543Z",
     "shell.execute_reply.started": "2025-09-14T09:54:08.981430Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your public URL: https://60288d3b0b6b.ngrok-free.app                                                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [36]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:38625 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.217.239.111:0 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "def free_port():\n",
    "    s = socket.socket()\n",
    "    s.bind(('', 0))\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "port = free_port()\n",
    "conf.get_default().auth_token = NGROK_TOKEN\n",
    "public_url = ngrok.connect(port).public_url\n",
    "print(\"Your public URL:\", public_url)\n",
    "\n",
    "def run(): uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
    "threading.Thread(target=run, daemon=True).start()\n",
    "time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8226444,
     "sourceId": 12996157,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8259543,
     "sourceId": 13043783,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
